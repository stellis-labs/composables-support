{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input', 'output', 'instruction'],\n",
       "     num_rows: 30559\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input', 'output', 'instruction'],\n",
       "     num_rows: 3396\n",
       " }))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset,val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'instruction'],\n",
       "        num_rows: 30559\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output', 'instruction'],\n",
       "        num_rows: 3396\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format prompts as per chat-style fine-tuning\n",
    "def format_prompt(example):\n",
    "    \"\"\"Format input as a ChatBot interaction (User ↔ Model).\"\"\"\n",
    "    prompt = f\"<|user|>\\n{example['instruction']}\\n\\n\"\n",
    "    \n",
    "    if example['input']:  # If context is available\n",
    "        prompt += f\"{example['input']}\\n\\n\"\n",
    "\n",
    "    prompt += \"<|model|>\\n\" + example[\"output\"]\n",
    "    return prompt\n",
    "\n",
    "# Select a few samples from the dataset and format them\n",
    "formatted_prompts = [{\"formatted_prompt\": format_prompt(sample)} for sample in dataset[\"test\"].select(range(10))]\n",
    "\n",
    "# Convert to DataFrame for display\n",
    "df_formatted_prompts = pd.DataFrame(formatted_prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|>\\nAnswer this question truthfully\\n\\nWhat type of injury to the arm/elbow most often leads to supracondylar fractures?\\n\\n<|model|>\\nSupracondylar fractures most often occur after hyperextension injuries of the arm/elbow.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_formatted_prompts.iloc[0]['formatted_prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Tokenization function (Fixed for Batched Processing)\n",
    "# def tokenize_function(examples):\n",
    "#     \"\"\"Tokenizes dataset examples in User ↔ Assistant format, ensuring correct type handling.\"\"\"\n",
    "    \n",
    "#     #  Ensure 'instruction' is processed correctly in batched mode\n",
    "#     instruction_texts = [ \" \".join(inst) if isinstance(inst, list) else inst for inst in examples[\"instruction\"] ]\n",
    "    \n",
    "#     #  Ensure 'input' (context) is processed correctly\n",
    "#     input_texts = [ \" \".join(inp) if isinstance(inp, list) else inp for inp in examples[\"input\"] ]\n",
    "    \n",
    "#     #  Ensure 'output' (response) is processed correctly\n",
    "#     output_texts = [ \" \".join(out) if isinstance(out, list) else out for out in examples[\"output\"] ]\n",
    "\n",
    "#     #  Format the conversation as a chatbot exchange\n",
    "#     prompts = []\n",
    "#     for instr, inp, out in zip(instruction_texts, input_texts, output_texts):\n",
    "#         prompt = f\"<|user|>\\n{instr.strip()}\"\n",
    "#         if inp.strip():\n",
    "#             prompt += f\"\\n{inp.strip()}\"\n",
    "#         prompt += f\"\\n\\n<|assistant|>\\n{out.strip()}\"\n",
    "#         prompts.append(prompt)\n",
    "\n",
    "#     #  Tokenize the formatted prompts\n",
    "#     tokenized = tokenizer(\n",
    "#         prompts,\n",
    "#         padding=\"max_length\",  # Ensures uniform batch size\n",
    "#         truncation=True,\n",
    "#         max_length=512\n",
    "#     )\n",
    "\n",
    "#     #  Convert tensors into lists to avoid TypeErrors\n",
    "#     tokenized[\"input_ids\"] = [ids.tolist() if isinstance(ids, torch.Tensor) else ids for ids in tokenized[\"input_ids\"]]\n",
    "#     tokenized[\"attention_mask\"] = [mask.tolist() if isinstance(mask, torch.Tensor) else mask for mask in tokenized[\"attention_mask\"]]\n",
    "\n",
    "#     #  Ensure labels match input_ids for causal LM training\n",
    "#     tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "\n",
    "#     return tokenized\n",
    "\n",
    "# #  Tokenize dataset with the fixed function\n",
    "# tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# # Assign tokenized datasets\n",
    "# train_dataset_tok = tokenized_datasets[\"train\"]\n",
    "# val_dataset_tok = tokenized_datasets[\"test\"]\n",
    "\n",
    "# # Take only the first 5 examples from the dataset for debugging\n",
    "# sample_data = dataset[\"train\"].select(range(5))\n",
    "\n",
    "# # Tokenize the selected samples\n",
    "# tokenized_samples = sample_data.map(tokenize_function, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# # Print properly formatted prompts for verification\n",
    "# for i, example in enumerate(sample_data):\n",
    "#     print(f\"🔹 **Sample {i+1} Prompt:**\")\n",
    "#     print(tokenize_function(example)[\"input_ids\"])  # Show tokenized input IDs\n",
    "#     print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset_tok[0],sample_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "c:\\Users\\harsh\\anaconda3\\envs\\special-llm\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_5280\\256145967.py:114: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting optimized training on 1000 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\anaconda3\\envs\\special-llm\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 18:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.435300</td>\n",
       "      <td>0.414540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.455100</td>\n",
       "      <td>0.402736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>0.396485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.469600</td>\n",
       "      <td>0.392564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.419200</td>\n",
       "      <td>0.390941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.387100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.400100</td>\n",
       "      <td>0.384891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.366400</td>\n",
       "      <td>0.383520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.409400</td>\n",
       "      <td>0.382544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.440700</td>\n",
       "      <td>0.380411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.410600</td>\n",
       "      <td>0.379506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.378876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.393200</td>\n",
       "      <td>0.377667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.440900</td>\n",
       "      <td>0.376931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.453200</td>\n",
       "      <td>0.376682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 **Training Log History:**\n",
      "[{'loss': 3.3203, 'grad_norm': 1.6854817867279053, 'learning_rate': 0.0004946666666666667, 'epoch': 0.013333333333333334, 'step': 10}, {'loss': 0.5523, 'grad_norm': 0.4944961965084076, 'learning_rate': 0.000488, 'epoch': 0.02666666666666667, 'step': 20}, {'loss': 0.5386, 'grad_norm': 0.8257476687431335, 'learning_rate': 0.00048133333333333334, 'epoch': 0.04, 'step': 30}, {'loss': 0.4996, 'grad_norm': 0.4939257502555847, 'learning_rate': 0.0004746666666666667, 'epoch': 0.05333333333333334, 'step': 40}, {'loss': 0.4353, 'grad_norm': 0.4518594443798065, 'learning_rate': 0.00046800000000000005, 'epoch': 0.06666666666666667, 'step': 50}, {'eval_loss': 0.4145403802394867, 'eval_runtime': 37.3167, 'eval_samples_per_second': 16.079, 'eval_steps_per_second': 4.02, 'epoch': 0.06666666666666667, 'step': 50}, {'loss': 0.4914, 'grad_norm': 0.5553702712059021, 'learning_rate': 0.00046133333333333334, 'epoch': 0.08, 'step': 60}, {'loss': 0.4304, 'grad_norm': 0.6684462428092957, 'learning_rate': 0.0004546666666666667, 'epoch': 0.09333333333333334, 'step': 70}, {'loss': 0.4211, 'grad_norm': 0.4753103256225586, 'learning_rate': 0.000448, 'epoch': 0.10666666666666667, 'step': 80}, {'loss': 0.3773, 'grad_norm': 0.27724888920783997, 'learning_rate': 0.00044133333333333335, 'epoch': 0.12, 'step': 90}, {'loss': 0.4551, 'grad_norm': 0.4849475920200348, 'learning_rate': 0.00043466666666666664, 'epoch': 0.13333333333333333, 'step': 100}, {'eval_loss': 0.40273594856262207, 'eval_runtime': 37.2557, 'eval_samples_per_second': 16.105, 'eval_steps_per_second': 4.026, 'epoch': 0.13333333333333333, 'step': 100}, {'loss': 0.4093, 'grad_norm': 0.43275028467178345, 'learning_rate': 0.000428, 'epoch': 0.14666666666666667, 'step': 110}, {'loss': 0.4382, 'grad_norm': 0.382282018661499, 'learning_rate': 0.00042133333333333335, 'epoch': 0.16, 'step': 120}, {'loss': 0.4498, 'grad_norm': 0.4251478314399719, 'learning_rate': 0.0004146666666666667, 'epoch': 0.17333333333333334, 'step': 130}, {'loss': 0.4692, 'grad_norm': 0.4172804057598114, 'learning_rate': 0.000408, 'epoch': 0.18666666666666668, 'step': 140}, {'loss': 0.4632, 'grad_norm': 0.4850311577320099, 'learning_rate': 0.00040133333333333335, 'epoch': 0.2, 'step': 150}, {'eval_loss': 0.3964850902557373, 'eval_runtime': 37.2772, 'eval_samples_per_second': 16.096, 'eval_steps_per_second': 4.024, 'epoch': 0.2, 'step': 150}, {'loss': 0.3331, 'grad_norm': 0.4836306571960449, 'learning_rate': 0.0003946666666666667, 'epoch': 0.21333333333333335, 'step': 160}, {'loss': 0.4126, 'grad_norm': 0.4654293954372406, 'learning_rate': 0.000388, 'epoch': 0.22666666666666666, 'step': 170}, {'loss': 0.4049, 'grad_norm': 0.4121323823928833, 'learning_rate': 0.00038133333333333335, 'epoch': 0.24, 'step': 180}, {'loss': 0.3992, 'grad_norm': 0.3797347843647003, 'learning_rate': 0.00037466666666666665, 'epoch': 0.25333333333333335, 'step': 190}, {'loss': 0.4696, 'grad_norm': 0.4510727822780609, 'learning_rate': 0.000368, 'epoch': 0.26666666666666666, 'step': 200}, {'eval_loss': 0.39256399869918823, 'eval_runtime': 37.479, 'eval_samples_per_second': 16.009, 'eval_steps_per_second': 4.002, 'epoch': 0.26666666666666666, 'step': 200}, {'loss': 0.4131, 'grad_norm': 0.5268154144287109, 'learning_rate': 0.00036133333333333335, 'epoch': 0.28, 'step': 210}, {'loss': 0.3694, 'grad_norm': 0.4091916084289551, 'learning_rate': 0.0003546666666666667, 'epoch': 0.29333333333333333, 'step': 220}, {'loss': 0.4172, 'grad_norm': 0.4834914803504944, 'learning_rate': 0.000348, 'epoch': 0.30666666666666664, 'step': 230}, {'loss': 0.5555, 'grad_norm': 0.5704411268234253, 'learning_rate': 0.00034133333333333335, 'epoch': 0.32, 'step': 240}, {'loss': 0.4192, 'grad_norm': 0.6029970049858093, 'learning_rate': 0.00033466666666666665, 'epoch': 0.3333333333333333, 'step': 250}, {'eval_loss': 0.39094093441963196, 'eval_runtime': 37.7149, 'eval_samples_per_second': 15.909, 'eval_steps_per_second': 3.977, 'epoch': 0.3333333333333333, 'step': 250}, {'loss': 0.3942, 'grad_norm': 0.48998868465423584, 'learning_rate': 0.000328, 'epoch': 0.3466666666666667, 'step': 260}, {'loss': 0.3795, 'grad_norm': 0.39286038279533386, 'learning_rate': 0.00032133333333333336, 'epoch': 0.36, 'step': 270}, {'loss': 0.4269, 'grad_norm': 0.4692125916481018, 'learning_rate': 0.00031466666666666665, 'epoch': 0.37333333333333335, 'step': 280}, {'loss': 0.3924, 'grad_norm': 0.3842964172363281, 'learning_rate': 0.000308, 'epoch': 0.38666666666666666, 'step': 290}, {'loss': 0.41, 'grad_norm': 0.36537396907806396, 'learning_rate': 0.00030133333333333336, 'epoch': 0.4, 'step': 300}, {'eval_loss': 0.3871004283428192, 'eval_runtime': 37.7125, 'eval_samples_per_second': 15.91, 'eval_steps_per_second': 3.977, 'epoch': 0.4, 'step': 300}, {'loss': 0.4236, 'grad_norm': 0.5260428786277771, 'learning_rate': 0.0002946666666666667, 'epoch': 0.41333333333333333, 'step': 310}, {'loss': 0.4468, 'grad_norm': 0.37723132967948914, 'learning_rate': 0.000288, 'epoch': 0.4266666666666667, 'step': 320}, {'loss': 0.4271, 'grad_norm': 0.4457147419452667, 'learning_rate': 0.0002813333333333333, 'epoch': 0.44, 'step': 330}, {'loss': 0.4038, 'grad_norm': 0.34845560789108276, 'learning_rate': 0.00027466666666666666, 'epoch': 0.4533333333333333, 'step': 340}, {'loss': 0.4001, 'grad_norm': 0.37695473432540894, 'learning_rate': 0.000268, 'epoch': 0.4666666666666667, 'step': 350}, {'eval_loss': 0.38489094376564026, 'eval_runtime': 37.7319, 'eval_samples_per_second': 15.902, 'eval_steps_per_second': 3.975, 'epoch': 0.4666666666666667, 'step': 350}, {'loss': 0.3876, 'grad_norm': 0.36095181107521057, 'learning_rate': 0.0002613333333333333, 'epoch': 0.48, 'step': 360}, {'loss': 0.4294, 'grad_norm': 0.4173021912574768, 'learning_rate': 0.00025466666666666666, 'epoch': 0.49333333333333335, 'step': 370}, {'loss': 0.4321, 'grad_norm': 0.46660783886909485, 'learning_rate': 0.000248, 'epoch': 0.5066666666666667, 'step': 380}, {'loss': 0.4199, 'grad_norm': 0.520504355430603, 'learning_rate': 0.00024133333333333334, 'epoch': 0.52, 'step': 390}, {'loss': 0.3664, 'grad_norm': 0.3255853056907654, 'learning_rate': 0.00023466666666666666, 'epoch': 0.5333333333333333, 'step': 400}, {'eval_loss': 0.3835204243659973, 'eval_runtime': 37.7207, 'eval_samples_per_second': 15.906, 'eval_steps_per_second': 3.977, 'epoch': 0.5333333333333333, 'step': 400}, {'loss': 0.3962, 'grad_norm': 0.5677307844161987, 'learning_rate': 0.000228, 'epoch': 0.5466666666666666, 'step': 410}, {'loss': 0.4205, 'grad_norm': 0.4177305996417999, 'learning_rate': 0.00022133333333333334, 'epoch': 0.56, 'step': 420}, {'loss': 0.4073, 'grad_norm': 0.36836549639701843, 'learning_rate': 0.0002146666666666667, 'epoch': 0.5733333333333334, 'step': 430}, {'loss': 0.3444, 'grad_norm': 0.4898540675640106, 'learning_rate': 0.000208, 'epoch': 0.5866666666666667, 'step': 440}, {'loss': 0.4094, 'grad_norm': 0.44595006108283997, 'learning_rate': 0.00020133333333333334, 'epoch': 0.6, 'step': 450}, {'eval_loss': 0.3825444281101227, 'eval_runtime': 37.7192, 'eval_samples_per_second': 15.907, 'eval_steps_per_second': 3.977, 'epoch': 0.6, 'step': 450}, {'loss': 0.4004, 'grad_norm': 0.5109313130378723, 'learning_rate': 0.00019466666666666666, 'epoch': 0.6133333333333333, 'step': 460}, {'loss': 0.4252, 'grad_norm': 0.49546146392822266, 'learning_rate': 0.00018800000000000002, 'epoch': 0.6266666666666667, 'step': 470}, {'loss': 0.4485, 'grad_norm': 0.3719155788421631, 'learning_rate': 0.00018133333333333334, 'epoch': 0.64, 'step': 480}, {'loss': 0.4212, 'grad_norm': 0.39273181557655334, 'learning_rate': 0.00017466666666666667, 'epoch': 0.6533333333333333, 'step': 490}, {'loss': 0.4407, 'grad_norm': 0.49915415048599243, 'learning_rate': 0.00016800000000000002, 'epoch': 0.6666666666666666, 'step': 500}, {'eval_loss': 0.3804105520248413, 'eval_runtime': 37.202, 'eval_samples_per_second': 16.128, 'eval_steps_per_second': 4.032, 'epoch': 0.6666666666666666, 'step': 500}, {'loss': 0.4307, 'grad_norm': 0.4615858197212219, 'learning_rate': 0.00016133333333333334, 'epoch': 0.68, 'step': 510}, {'loss': 0.4537, 'grad_norm': 0.32887089252471924, 'learning_rate': 0.00015466666666666667, 'epoch': 0.6933333333333334, 'step': 520}, {'loss': 0.3902, 'grad_norm': 0.43048253655433655, 'learning_rate': 0.000148, 'epoch': 0.7066666666666667, 'step': 530}, {'loss': 0.3911, 'grad_norm': 0.43559005856513977, 'learning_rate': 0.00014133333333333334, 'epoch': 0.72, 'step': 540}, {'loss': 0.4106, 'grad_norm': 0.4120450019836426, 'learning_rate': 0.00013466666666666667, 'epoch': 0.7333333333333333, 'step': 550}, {'eval_loss': 0.379506379365921, 'eval_runtime': 37.5789, 'eval_samples_per_second': 15.966, 'eval_steps_per_second': 3.992, 'epoch': 0.7333333333333333, 'step': 550}, {'loss': 0.4386, 'grad_norm': 0.5821948647499084, 'learning_rate': 0.000128, 'epoch': 0.7466666666666667, 'step': 560}, {'loss': 0.4209, 'grad_norm': 0.479210764169693, 'learning_rate': 0.00012133333333333333, 'epoch': 0.76, 'step': 570}, {'loss': 0.4593, 'grad_norm': 0.507447361946106, 'learning_rate': 0.00011466666666666667, 'epoch': 0.7733333333333333, 'step': 580}, {'loss': 0.409, 'grad_norm': 0.5333164930343628, 'learning_rate': 0.000108, 'epoch': 0.7866666666666666, 'step': 590}, {'loss': 0.3823, 'grad_norm': 0.4638442397117615, 'learning_rate': 0.00010133333333333333, 'epoch': 0.8, 'step': 600}, {'eval_loss': 0.3788762390613556, 'eval_runtime': 37.7432, 'eval_samples_per_second': 15.897, 'eval_steps_per_second': 3.974, 'epoch': 0.8, 'step': 600}, {'loss': 0.3626, 'grad_norm': 0.4748719334602356, 'learning_rate': 9.466666666666666e-05, 'epoch': 0.8133333333333334, 'step': 610}, {'loss': 0.3903, 'grad_norm': 0.40361452102661133, 'learning_rate': 8.8e-05, 'epoch': 0.8266666666666667, 'step': 620}, {'loss': 0.3917, 'grad_norm': 0.5208368301391602, 'learning_rate': 8.133333333333332e-05, 'epoch': 0.84, 'step': 630}, {'loss': 0.4012, 'grad_norm': 0.4946712553501129, 'learning_rate': 7.466666666666667e-05, 'epoch': 0.8533333333333334, 'step': 640}, {'loss': 0.3932, 'grad_norm': 0.3574484586715698, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.8666666666666667, 'step': 650}, {'eval_loss': 0.37766724824905396, 'eval_runtime': 37.6926, 'eval_samples_per_second': 15.918, 'eval_steps_per_second': 3.98, 'epoch': 0.8666666666666667, 'step': 650}, {'loss': 0.412, 'grad_norm': 0.403677374124527, 'learning_rate': 6.133333333333334e-05, 'epoch': 0.88, 'step': 660}, {'loss': 0.3952, 'grad_norm': 0.4178307354450226, 'learning_rate': 5.466666666666667e-05, 'epoch': 0.8933333333333333, 'step': 670}, {'loss': 0.4041, 'grad_norm': 0.6302539706230164, 'learning_rate': 4.8e-05, 'epoch': 0.9066666666666666, 'step': 680}, {'loss': 0.3962, 'grad_norm': 0.5078620910644531, 'learning_rate': 4.133333333333333e-05, 'epoch': 0.92, 'step': 690}, {'loss': 0.4409, 'grad_norm': 0.421546071767807, 'learning_rate': 3.4666666666666665e-05, 'epoch': 0.9333333333333333, 'step': 700}, {'eval_loss': 0.3769305944442749, 'eval_runtime': 37.7226, 'eval_samples_per_second': 15.906, 'eval_steps_per_second': 3.976, 'epoch': 0.9333333333333333, 'step': 700}, {'loss': 0.3609, 'grad_norm': 0.4550509452819824, 'learning_rate': 2.8e-05, 'epoch': 0.9466666666666667, 'step': 710}, {'loss': 0.4054, 'grad_norm': 0.3738975524902344, 'learning_rate': 2.1333333333333335e-05, 'epoch': 0.96, 'step': 720}, {'loss': 0.3242, 'grad_norm': 0.35016098618507385, 'learning_rate': 1.4666666666666666e-05, 'epoch': 0.9733333333333334, 'step': 730}, {'loss': 0.402, 'grad_norm': 0.4546881914138794, 'learning_rate': 8e-06, 'epoch': 0.9866666666666667, 'step': 740}, {'loss': 0.4532, 'grad_norm': 0.42743679881095886, 'learning_rate': 1.3333333333333334e-06, 'epoch': 1.0, 'step': 750}, {'eval_loss': 0.37668177485466003, 'eval_runtime': 37.3058, 'eval_samples_per_second': 16.083, 'eval_steps_per_second': 4.021, 'epoch': 1.0, 'step': 750}, {'train_runtime': 1097.972, 'train_samples_per_second': 2.732, 'train_steps_per_second': 0.683, 'total_flos': 5266929681530880.0, 'train_loss': 0.45757060845692954, 'epoch': 1.0, 'step': 750}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "\n",
    "#  Load Model and Tokenizer\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Use EOS as PAD token (Fix for LLaMA padding issue)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Select Single Device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#  Load Model on Single Device with 4-bit QLoRA (Super Efficient)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_4bit=True,  #  More memory-efficient than 8-bit\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "#  Prepare model for QLoRA fine-tuning (Memory Efficient)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "#  Apply LoRA Configuration (Optimized)\n",
    "peft_config = LoraConfig(\n",
    "    r=8,  \n",
    "    lora_alpha=16,  \n",
    "    lora_dropout=0.05,  \n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "#  Apply LoRA to Model\n",
    "model = get_peft_model(model, peft_config).to(device)\n",
    "\n",
    "# Enable Gradient Checkpointing for Memory Savings\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Load and split the dataset into train/validation (90:10 split)\n",
    "dataset = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]\n",
    "\n",
    "# Tokenization function (Optimized)\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenizes dataset in User ↔ Assistant format, ensuring correct type handling.\"\"\"\n",
    "    \n",
    "    instruction_texts = [\" \".join(inst) if isinstance(inst, list) else inst for inst in examples[\"instruction\"]]\n",
    "    input_texts = [\" \".join(inp) if isinstance(inp, list) else inp for inp in examples[\"input\"]]\n",
    "    output_texts = [\" \".join(out) if isinstance(out, list) else out for out in examples[\"output\"]]\n",
    "\n",
    "    prompts = []\n",
    "    for instr, inp, out in zip(instruction_texts, input_texts, output_texts):\n",
    "        prompt = f\"<|user|>\\n{instr.strip()}\"\n",
    "        if inp.strip():\n",
    "            prompt += f\"\\n{inp.strip()}\"\n",
    "        prompt += f\"\\n\\n<|assistant|>\\n{out.strip()}\"\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    # Tokenize efficiently\n",
    "    tokenized = tokenizer(\n",
    "        prompts,\n",
    "        padding=\"longest\",  # Efficient batch processing\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    #  Ensure labels match input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "#  Tokenize dataset (Faster processing)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "train_dataset_tok = tokenized_datasets[\"train\"]\n",
    "val_dataset_tok = tokenized_datasets[\"test\"]\n",
    "\n",
    "#  Select First 1000 Samples for Training, 200 for Validation (Faster Debugging)\n",
    "small_train = train_dataset_tok.select(range(3000))\n",
    "small_val = val_dataset_tok.select(range(600))\n",
    "\n",
    "#  Data Collator with Optimized Padding\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer, \n",
    "    model=model, \n",
    "    padding=\"longest\"  #  More efficient padding\n",
    ")\n",
    "\n",
    "# Optimized Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-medical-chatbot\",\n",
    "    per_device_train_batch_size=4,  #  Increased batch size\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=50,  # Less frequent evaluation\n",
    "    save_steps=50,  # Less frequent saving\n",
    "    logging_steps=10,  # Log progress every 10 steps\n",
    "    learning_rate=5e-4,\n",
    "    num_train_epochs=1,  # 1 Epoch for fast debugging\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,  # Mixed precision training for speedup\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# Trainer Setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=small_val,  \n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "print(\"🚀 Starting optimized training on 1000 samples...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./llama3-medical-chatbot\")\n",
    "tokenizer.save_pretrained(\"./llama3-medical-chatbot\")\n",
    "\n",
    "# Debugging: Check Training Progress\n",
    "print(\"\\n🔍 **Training Log History:**\")\n",
    "print(trainer.state.log_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 **Medical Chatbot Response:**\n",
      "<|user|>\n",
      "What are the symptoms and treatments for apnea?\n",
      "\n",
      "<|assistant|>\n",
      "Apnea is a condition where a person's breathing stops for a short period of time. There are several possible causes of apnea, including sleep apnea, which is a more serious condition that can lead to other health problems if not treated. Treatment for apnea may involve using a CPAP machine, which helps to keep the airways open during sleep, or using other devices to prevent apnea. In some cases, surgery may be necessary to correct the underlying cause of apnea.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load the fine-tuned model path\n",
    "model_path = \"./llama3-medical-chatbot\"  # Path where fine-tuned model is saved\n",
    "base_model_id = \"meta-llama/Llama-3.2-1B\"  # Original base model\n",
    "\n",
    "# Load the tokenizer from the fine-tuned model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "#  Ensure the tokenizer has a pad token to avoid warnings\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Avoids padding issues\n",
    "\n",
    "# Select device (Prefer GPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load the fine-tuned LoRA adapter on top of the base model\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "\n",
    "# Ensure model embeddings match tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move model to correct device\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def generate_response(user_query, max_length=256):\n",
    "    \"\"\"Generates a medical response using the fine-tuned chatbot.\"\"\"\n",
    "    \n",
    "    # Format the query using Chat-based structure\n",
    "    prompt = f\"<|user|>\\n{user_query}\\n\\n<|assistant|>\\n\"\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)  # Explicitly pass attention mask\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,  # Fixes warning\n",
    "            max_length=max_length,\n",
    "            do_sample=True,  # Enable randomness for diverse responses\n",
    "            temperature=0.4,  # Lower temp = more factual response\n",
    "            top_p=0.9,  # Controls response diversity\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# **Test the chatbot with a medical query**\n",
    "query = \"What are the symptoms and treatments for apnea?\"\n",
    "response = generate_response(query)\n",
    "\n",
    "print(\"\\n🔍 **Medical Chatbot Response:**\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 **Base Model Response:**\n",
      "<|user|>\n",
      "What are the symptoms and treatments for apnea?\n",
      "\n",
      "<|assistant|>\n",
      "The most common symptoms of apnea are:\n",
      "• Unexplained sleepiness or drowsiness during the day\n",
      "• Unexplained fatigue or weakness\n",
      "• Unexplained headaches or migraines\n",
      "• Unexplained weight gain or weight loss\n",
      "• Unexplained changes in mood or behavior\n",
      "• Unexplained changes in appetite or eating habits\n",
      "• Unexplained changes in sleep habits or patterns\n",
      "• Unexplained changes in sex drive or libido\n",
      "• Unexplained changes in sexual performance or ability\n",
      "• Unexplained changes in energy levels or stamina\n",
      "• Unexplained changes in concentration or focus\n",
      "• Unexplained changes in memory or cognitive function\n",
      "• Unexplained changes in attention span or focus\n",
      "• Unexplained changes in reaction time or reaction speed\n",
      "• Unexplained changes in judgment or decision-making ability\n",
      "• Unexplained changes in emotional stability or emotional outbursts\n",
      "• Unexplained changes in personality or personality traits\n",
      "• Unexplained changes in social behavior or social interactions\n",
      "• Unexplained changes in social anxiety or social phobia\n",
      "• Unexplained changes in social skills or social interactions\n",
      "• Unexplained changes in social relationships or social interactions\n",
      "• Unexplained changes in social support or social interactions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the base model\n",
    "base_model_id = \"meta-llama/Llama-3.2-1B\"  # Original LLaMA-3.2 1B model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "\n",
    "# Ensure tokenizer has a pad token to avoid warnings\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Avoid padding issues\n",
    "\n",
    "#  Select device (Prefer GPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the base model on the chosen device\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ").to(device)\n",
    "\n",
    "def generate_response_base(user_query, max_length=256):\n",
    "    \"\"\"Generates a response using the base LLaMA-3 model (without fine-tuning).\"\"\"\n",
    "\n",
    "    # Format the query in Chat-style prompt\n",
    "    prompt = f\"<|user|>\\n{user_query}\\n\\n<|assistant|>\\n\"\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)  #  Explicitly pass attention mask\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,  # Fixes warning\n",
    "            max_length=max_length,\n",
    "            do_sample=True,  # Enable randomness for diverse responses\n",
    "            temperature=0.4,  # Lower temp = more factual response\n",
    "            top_p=0.9,  # Controls response diversity\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# **Test the base model with the same medical query**\n",
    "query = \"What are the symptoms and treatments for apnea?\"\n",
    "response_base = generate_response_base(query)\n",
    "\n",
    "print(\"\\n🔍 **Base Model Response:**\")\n",
    "print(response_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Required for BLEU score tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Benchmarking Base Model...\n",
      "🚀 Benchmarking LoRA Model...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load Fine-tuned LoRA Model\n",
    "fine_tuned_model_path = \"./llama3-medical-chatbot\"\n",
    "base_model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token \n",
    "# Load base model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id).to(device)\n",
    "\n",
    "# Load fine-tuned LoRA model\n",
    "fine_tuned_model = PeftModel.from_pretrained(base_model, fine_tuned_model_path).to(device)\n",
    "\n",
    "# Load Benchmark Dataset\n",
    "dataset = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")[\"train\"]\n",
    "benchmark_dataset = dataset.select(range(3600, 4000))  # Data outside training\n",
    "\n",
    "# Load Sentence Transformer for Semantic Similarity\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Benchmark Function\n",
    "def benchmark_model(model, queries, tokenizer, model_name=\"LoRA\"):\n",
    "    results = []\n",
    "    \n",
    "    for query in queries:\n",
    "        prompt = f\"<|user|>\\n{query}\\n\\n<|assistant|>\\n\"\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "        # Measure inference time\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=256,\n",
    "                do_sample=True,\n",
    "                temperature=0.4,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        end_time = time.time()\n",
    "\n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        # Compute Inference Time & Token Speed\n",
    "        inference_time = end_time - start_time\n",
    "        token_count = output.shape[1]  # Number of tokens generated\n",
    "        tokens_per_second = token_count / inference_time if inference_time > 0 else 0\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Query\": query,\n",
    "            \"Response\": response,\n",
    "            \"Inference Time (s)\": round(inference_time, 4),\n",
    "            \"Tokens Per Second\": round(tokens_per_second, 2)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run Benchmark for Both Models\n",
    "benchmark_queries = [\n",
    "    f\"{instr} {inp}\" if inp.strip() else instr \n",
    "    for instr, inp in zip(benchmark_dataset[\"instruction\"], benchmark_dataset[\"input\"])\n",
    "][:100]  # Take first 100 samples\n",
    "\n",
    "print(\"🚀 Benchmarking Base Model...\")\n",
    "base_results = benchmark_model(base_model, benchmark_queries, tokenizer, model_name=\"Base\")\n",
    "\n",
    "print(\"🚀 Benchmarking LoRA Model...\")\n",
    "lora_results = benchmark_model(fine_tuned_model, benchmark_queries, tokenizer, model_name=\"LoRA Fine-tuned\")\n",
    "\n",
    "# Convert Results to DataFrame\n",
    "df_base = pd.DataFrame(base_results)\n",
    "df_lora = pd.DataFrame(lora_results)\n",
    "\n",
    "# Combine and Display Results\n",
    "df_benchmark = pd.concat([df_base, df_lora], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.rename(columns={\"Response\": \"Base Response\", \"Inference Time (s)\": \"Base Time\", \"Tokens Per Second\": \"Base Tokens/sec\"}, inplace=True)\n",
    "df_lora.rename(columns={\"Response\": \"LoRA Response\", \"Inference Time (s)\": \"LoRA Time\", \"Tokens Per Second\": \"LoRA Tokens/sec\"}, inplace=True)\n",
    "\n",
    "df_benchmark = df_base.merge(df_lora, on=\"Query\", suffixes=(\"_Base\", \"_LoRA\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# csv_file = \"time_benchmark.csv\"\n",
    "# try:\n",
    "#     df_benchmark  # Check if df_benchmark exists\n",
    "# except NameError:\n",
    "#     if os.path.exists(csv_file):\n",
    "#         df_benchmark = pd.read_csv(csv_file)  # Load data from CSV if the file exists\n",
    "#     else:\n",
    "#         df_benchmark = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "# else:\n",
    "#     df_benchmark.to_csv(csv_file, index=False)  # Save DataFrame if it already exists\n",
    "\n",
    "# df_benchmark.drop(columns=[\"Model_Base\",\"Model_LoRA\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\anaconda3\\envs\\special-llm\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\harsh\\anaconda3\\envs\\special-llm\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\harsh\\anaconda3\\envs\\special-llm\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\harsh\\anaconda3\\envs\\special-llm\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\harsh\\anaconda3\\envs\\special-llm\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\harsh\\anaconda3\\envs\\special-llm\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute BLEU Scores\n",
    "def compute_bleu(responses, references):\n",
    "    scores = []\n",
    "    for response, reference in zip(responses, references):\n",
    "        scores.append(sentence_bleu([reference.split()], response.split()))\n",
    "    return scores\n",
    "\n",
    "#  Compute Embedding Similarity\n",
    "def compute_embedding_similarity(responses, references, model):\n",
    "    response_embeddings = model.encode(responses)\n",
    "    reference_embeddings = model.encode(references)\n",
    "    \n",
    "    similarities = []\n",
    "    for resp_emb, ref_emb in zip(response_embeddings, reference_embeddings):\n",
    "        similarity = cosine_similarity([resp_emb], [ref_emb])[0][0]\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "#  Extract Ground Truth Answers\n",
    "ground_truth = benchmark_dataset[\"output\"]#[:100]  # Get ground truth answers\n",
    "\n",
    "def extract_assistant_response(output_text):\n",
    "    \"\"\"Removes the prompt and extracts only the assistant's response.\"\"\"\n",
    "    if \"<|assistant|>\" in output_text:\n",
    "        return output_text.split(\"<|assistant|>\\n\", 1)[1].strip()  # Keep only text after <|assistant|> and remove extra spaces\n",
    "    return output_text.strip()  # If no <|assistant|> tag, just strip whitespace\n",
    "\n",
    "#  Apply Cleaning on Existing DataFrame\n",
    "df_benchmark[\"Base Response Clean\"] = df_benchmark[\"Base Response\"].apply(extract_assistant_response)\n",
    "df_benchmark[\"LoRA Response Clean\"] = df_benchmark[\"LoRA Response\"].apply(extract_assistant_response)\n",
    "\n",
    "\n",
    "#  Compute BLEU Scores for Base and LoRA Responses\n",
    "df_benchmark[\"BLEU Score (Base)\"] = compute_bleu(df_benchmark[\"Base Response\"], ground_truth)\n",
    "df_benchmark[\"BLEU Score (LoRA)\"] = compute_bleu(df_benchmark[\"LoRA Response\"], ground_truth)\n",
    "\n",
    "# Compute Embedding Similarity for Base and LoRA Responses\n",
    "df_benchmark[\"Embedding Similarity (Base)\"] = compute_embedding_similarity(df_benchmark[\"Base Response\"], ground_truth, embedding_model)\n",
    "df_benchmark[\"Embedding Similarity (LoRA)\"] = compute_embedding_similarity(df_benchmark[\"LoRA Response\"], ground_truth, embedding_model)\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Compute ROUGE-L Score\n",
    "def compute_rouge_l(responses, references):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "    scores = [scorer.score(ref, resp)[\"rougeL\"].fmeasure for resp, ref in zip(responses, references)]\n",
    "    return scores\n",
    "\n",
    "# Compute Perplexity\n",
    "def compute_perplexity(model, tokenizer, texts):\n",
    "    perplexities = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            log_probs = F.log_softmax(outputs.logits, dim=-1)\n",
    "            perplexity = torch.exp(-log_probs.mean())\n",
    "        perplexities.append(perplexity.item())\n",
    "    return perplexities\n",
    "\n",
    "# Add Ground Truth to df_benchmark\n",
    "df_benchmark[\"Ground Truth\"] = benchmark_dataset[\"output\"][:len(df_benchmark)]\n",
    "\n",
    "\n",
    "# Compute ROUGE-L Scores for Base and LoRA\n",
    "df_benchmark[\"ROUGE-L (Base)\"] = compute_rouge_l(df_benchmark[\"Base Response Clean\"], df_benchmark[\"Ground Truth\"])\n",
    "df_benchmark[\"ROUGE-L (LoRA)\"] = compute_rouge_l(df_benchmark[\"LoRA Response Clean\"], df_benchmark[\"Ground Truth\"])\n",
    "\n",
    "# Compute Perplexity for Base and LoRA\n",
    "df_benchmark[\"Perplexity (Base)\"] = compute_perplexity(base_model, tokenizer, df_benchmark[\"Base Response Clean\"])\n",
    "df_benchmark[\"Perplexity (LoRA)\"] = compute_perplexity(fine_tuned_model, tokenizer, df_benchmark[\"LoRA Response Clean\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Gestational hypertension is a condition that occurs during pregnancy that can lead to high blood pressure and other complications. It is defined as a blood pressure of 140/90 mmHg or higher during the second or third trimester of pregnancy. Gestational hypertension can be caused by a variety of factors, including hormonal changes, changes in the body's blood vessels, and changes in the placenta. It is important to monitor blood pressure during pregnancy, and to seek medical attention if it is not under control. Treatment may include lifestyle changes, medication, and in some cases, surgery.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_benchmark['Base Response Clean'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benchmark.to_csv(\"benchmark_res.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 81:\n",
      "--------------------------------------------------\n",
      "🔹 Query:\n",
      "Answer this question truthfully What is fenoldopam, and how does it work to prevent kidney damage in patients with hypertensive emergency? What is the role of the D1 receptor in this process, and what are some potential benefits and risks associated with the use of fenoldopam?\n",
      "🔹 Base Response:\n",
      "Fenoldopam is a vasopressor drug that works by inhibiting the D1 receptor, which is involved in the regulation of blood pressure. In patients with hypertensive emergency, fenoldopam can help to prevent kidney damage by reducing blood pressure and improving blood flow to the kidneys. However, fenoldopam can also cause side effects such as hypotension, bradycardia, and flushing. Therefore, it is important to monitor patients closely for signs of hypotension and bradycardia, and to adjust the dosage as needed to avoid these side effects.\n",
      "🔹 LoRA Response:\n",
      "Fenoldopam is a medication that is used to prevent kidney damage in patients with hypertensive emergency. It works by blocking the D1 receptor, which is responsible for the release of renin, a hormone that causes the kidneys to contract and increase blood pressure. By blocking the D1 receptor, fenoldopam prevents the release of renin, which helps to reduce blood pressure and prevent further damage to the kidneys. However, fenoldopam can also cause side effects such as bradycardia, hypotension, and tachycardia, which can be managed with appropriate monitoring and treatment.\n",
      "🔹 Ground Truth:\n",
      "Fenoldopam is a medication that acts as a D1 receptor agonist, and it may be used in the treatment of hypertensive emergency to prevent kidney damage. The D1 receptor plays a role in regulating renal blood flow and sodium excretion, and activation of this receptor by fenoldopam can help to improve renal function in patients with hypertensive emergency. Some potential benefits of fenoldopam include its rapid onset of action and ability to lower blood pressure without causing significant changes in heart rate or cardiac output. However, fenoldopam may also be associated with side effects such as headache, flushing, and hypotension, and should be used with caution in patients with a history of heart failure or renal impairment.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Select 10 random samples from df_benchmark\n",
    "df_random_samples = df_benchmark[[\"Query\",\"Base Response Clean\", \"LoRA Response Clean\", \"Ground Truth\"]].sample(n=1, random_state=1)\n",
    "\n",
    "# Print full text for each selected sample\n",
    "for index, row in df_random_samples.iterrows():\n",
    "    print(f\"\\nSample {index+1}:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"🔹 Query:\\n{row['Query']}\")\n",
    "    print(f\"🔹 Base Response:\\n{row['Base Response Clean']}\")\n",
    "    print(f\"🔹 LoRA Response:\\n{row['LoRA Response Clean']}\")\n",
    "    print(f\"🔹 Ground Truth:\\n{row['Ground Truth']}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Base Response Clean</th>\n",
       "      <th>LoRA Response Clean</th>\n",
       "      <th>Ground Truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Answer this question truthfully What is fenold...</td>\n",
       "      <td>Fenoldopam is a vasopressor drug that works by...</td>\n",
       "      <td>Fenoldopam is a medication that is used to pre...</td>\n",
       "      <td>Fenoldopam is a medication that acts as a D1 r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Query  \\\n",
       "80  Answer this question truthfully What is fenold...   \n",
       "\n",
       "                                  Base Response Clean  \\\n",
       "80  Fenoldopam is a vasopressor drug that works by...   \n",
       "\n",
       "                                  LoRA Response Clean  \\\n",
       "80  Fenoldopam is a medication that is used to pre...   \n",
       "\n",
       "                                         Ground Truth  \n",
       "80  Fenoldopam is a medication that acts as a D1 r...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_random_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_Base</th>\n",
       "      <th>Query</th>\n",
       "      <th>Base Response</th>\n",
       "      <th>Base Time</th>\n",
       "      <th>Base Tokens/sec</th>\n",
       "      <th>Model_LoRA</th>\n",
       "      <th>LoRA Response</th>\n",
       "      <th>LoRA Time</th>\n",
       "      <th>LoRA Tokens/sec</th>\n",
       "      <th>Base Response Clean</th>\n",
       "      <th>LoRA Response Clean</th>\n",
       "      <th>BLEU Score (Base)</th>\n",
       "      <th>BLEU Score (LoRA)</th>\n",
       "      <th>Embedding Similarity (Base)</th>\n",
       "      <th>Embedding Similarity (LoRA)</th>\n",
       "      <th>Ground Truth</th>\n",
       "      <th>ROUGE-L (Base)</th>\n",
       "      <th>ROUGE-L (LoRA)</th>\n",
       "      <th>Perplexity (Base)</th>\n",
       "      <th>Perplexity (LoRA)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base</td>\n",
       "      <td>Answer this question truthfully What is gestat...</td>\n",
       "      <td>&lt;|user|&gt;\\nAnswer this question truthfully What...</td>\n",
       "      <td>4.1923</td>\n",
       "      <td>34.83</td>\n",
       "      <td>LoRA Fine-tuned</td>\n",
       "      <td>&lt;|user|&gt;\\nAnswer this question truthfully What...</td>\n",
       "      <td>3.8634</td>\n",
       "      <td>44.26</td>\n",
       "      <td>Gestational hypertension is a condition that o...</td>\n",
       "      <td>Gestational hypertension is a condition that o...</td>\n",
       "      <td>0.100785</td>\n",
       "      <td>0.106908</td>\n",
       "      <td>0.895864</td>\n",
       "      <td>0.905449</td>\n",
       "      <td>Gestational hypertension is a type of high blo...</td>\n",
       "      <td>0.301676</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>5.227424e+10</td>\n",
       "      <td>8.854676e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model_Base                                              Query  \\\n",
       "0       Base  Answer this question truthfully What is gestat...   \n",
       "\n",
       "                                       Base Response  Base Time  \\\n",
       "0  <|user|>\\nAnswer this question truthfully What...     4.1923   \n",
       "\n",
       "   Base Tokens/sec       Model_LoRA  \\\n",
       "0            34.83  LoRA Fine-tuned   \n",
       "\n",
       "                                       LoRA Response  LoRA Time  \\\n",
       "0  <|user|>\\nAnswer this question truthfully What...     3.8634   \n",
       "\n",
       "   LoRA Tokens/sec                                Base Response Clean  \\\n",
       "0            44.26  Gestational hypertension is a condition that o...   \n",
       "\n",
       "                                 LoRA Response Clean  BLEU Score (Base)  \\\n",
       "0  Gestational hypertension is a condition that o...           0.100785   \n",
       "\n",
       "   BLEU Score (LoRA)  Embedding Similarity (Base)  \\\n",
       "0           0.106908                     0.895864   \n",
       "\n",
       "   Embedding Similarity (LoRA)  \\\n",
       "0                     0.905449   \n",
       "\n",
       "                                        Ground Truth  ROUGE-L (Base)  \\\n",
       "0  Gestational hypertension is a type of high blo...        0.301676   \n",
       "\n",
       "   ROUGE-L (LoRA)  Perplexity (Base)  Perplexity (LoRA)  \n",
       "0        0.285714       5.227424e+10       8.854676e+10  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_benchmark.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics for all numeric columns in df_benchmark\n",
    "df_stats = df_benchmark.describe().round(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Base Time</th>\n",
       "      <th>Base Tokens/sec</th>\n",
       "      <th>LoRA Time</th>\n",
       "      <th>LoRA Tokens/sec</th>\n",
       "      <th>BLEU Score (Base)</th>\n",
       "      <th>BLEU Score (LoRA)</th>\n",
       "      <th>Embedding Similarity (Base)</th>\n",
       "      <th>Embedding Similarity (LoRA)</th>\n",
       "      <th>ROUGE-L (Base)</th>\n",
       "      <th>ROUGE-L (LoRA)</th>\n",
       "      <th>Perplexity (Base)</th>\n",
       "      <th>Perplexity (LoRA)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.0000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.4180</td>\n",
       "      <td>59.3243</td>\n",
       "      <td>2.3080</td>\n",
       "      <td>66.5116</td>\n",
       "      <td>0.0925</td>\n",
       "      <td>0.0905</td>\n",
       "      <td>0.7665</td>\n",
       "      <td>0.7564</td>\n",
       "      <td>0.2913</td>\n",
       "      <td>0.2907</td>\n",
       "      <td>2.324731e+10</td>\n",
       "      <td>2.227180e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.4872</td>\n",
       "      <td>20.7117</td>\n",
       "      <td>1.5684</td>\n",
       "      <td>22.9522</td>\n",
       "      <td>0.0705</td>\n",
       "      <td>0.0719</td>\n",
       "      <td>0.1115</td>\n",
       "      <td>0.1147</td>\n",
       "      <td>0.1149</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>2.473445e+10</td>\n",
       "      <td>2.286161e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.3812</td>\n",
       "      <td>29.7800</td>\n",
       "      <td>0.3079</td>\n",
       "      <td>43.1300</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4421</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.0882</td>\n",
       "      <td>0.0492</td>\n",
       "      <td>4.034423e+08</td>\n",
       "      <td>1.272360e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.9156</td>\n",
       "      <td>47.0725</td>\n",
       "      <td>0.6928</td>\n",
       "      <td>47.5300</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>0.7083</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.2268</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>2.758798e+09</td>\n",
       "      <td>1.973241e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.4708</td>\n",
       "      <td>51.4450</td>\n",
       "      <td>2.3836</td>\n",
       "      <td>55.1050</td>\n",
       "      <td>0.0790</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>0.7883</td>\n",
       "      <td>0.7755</td>\n",
       "      <td>0.2831</td>\n",
       "      <td>0.2785</td>\n",
       "      <td>1.573688e+10</td>\n",
       "      <td>1.786844e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.3438</td>\n",
       "      <td>73.6800</td>\n",
       "      <td>3.5354</td>\n",
       "      <td>84.5075</td>\n",
       "      <td>0.1222</td>\n",
       "      <td>0.1202</td>\n",
       "      <td>0.8577</td>\n",
       "      <td>0.8445</td>\n",
       "      <td>0.3204</td>\n",
       "      <td>0.3273</td>\n",
       "      <td>3.577883e+10</td>\n",
       "      <td>3.666467e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.9843</td>\n",
       "      <td>111.7500</td>\n",
       "      <td>5.9360</td>\n",
       "      <td>139.6800</td>\n",
       "      <td>0.3407</td>\n",
       "      <td>0.3407</td>\n",
       "      <td>0.9146</td>\n",
       "      <td>0.9278</td>\n",
       "      <td>0.7778</td>\n",
       "      <td>0.7778</td>\n",
       "      <td>1.205956e+11</td>\n",
       "      <td>9.892247e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Base Time  Base Tokens/sec  LoRA Time  LoRA Tokens/sec  \\\n",
       "count   100.0000         100.0000   100.0000         100.0000   \n",
       "mean      2.4180          59.3243     2.3080          66.5116   \n",
       "std       1.4872          20.7117     1.5684          22.9522   \n",
       "min       0.3812          29.7800     0.3079          43.1300   \n",
       "25%       0.9156          47.0725     0.6928          47.5300   \n",
       "50%       2.4708          51.4450     2.3836          55.1050   \n",
       "75%       3.3438          73.6800     3.5354          84.5075   \n",
       "max       6.9843         111.7500     5.9360         139.6800   \n",
       "\n",
       "       BLEU Score (Base)  BLEU Score (LoRA)  Embedding Similarity (Base)  \\\n",
       "count           100.0000           100.0000                     100.0000   \n",
       "mean              0.0925             0.0905                       0.7665   \n",
       "std               0.0705             0.0719                       0.1115   \n",
       "min               0.0000             0.0000                       0.4421   \n",
       "25%               0.0430             0.0401                       0.7083   \n",
       "50%               0.0790             0.0900                       0.7883   \n",
       "75%               0.1222             0.1202                       0.8577   \n",
       "max               0.3407             0.3407                       0.9146   \n",
       "\n",
       "       Embedding Similarity (LoRA)  ROUGE-L (Base)  ROUGE-L (LoRA)  \\\n",
       "count                     100.0000        100.0000        100.0000   \n",
       "mean                        0.7564          0.2913          0.2907   \n",
       "std                         0.1147          0.1149          0.1177   \n",
       "min                         0.4624          0.0882          0.0492   \n",
       "25%                         0.6953          0.2268          0.2222   \n",
       "50%                         0.7755          0.2831          0.2785   \n",
       "75%                         0.8445          0.3204          0.3273   \n",
       "max                         0.9278          0.7778          0.7778   \n",
       "\n",
       "       Perplexity (Base)  Perplexity (LoRA)  \n",
       "count       1.000000e+02       1.000000e+02  \n",
       "mean        2.324731e+10       2.227180e+10  \n",
       "std         2.473445e+10       2.286161e+10  \n",
       "min         4.034423e+08       1.272360e+08  \n",
       "25%         2.758798e+09       1.973241e+09  \n",
       "50%         1.573688e+10       1.786844e+10  \n",
       "75%         3.577883e+10       3.666467e+10  \n",
       "max         1.205956e+11       9.892247e+10  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
