{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Can you provide an overview of the lung's squamous cell carcinoma? What are the most common symptoms?\\nIn squamous cell carcinoma of the lung, the tumor is usually\"}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "pipe(\"Can you provide an overview of the lung's squamous cell carcinoma?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available dataset splits: dict_keys(['train'])\n",
      "Train size: 9000, Validation size: 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c15dae17fed4a0da80190ea42a3fbe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841a1fe7bdf8437293fc7d8ecd4036d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\anaconda3\\envs\\special-llm\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_13996\\27437124.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "c:\\Users\\harsh\\anaconda3\\envs\\special-llm\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\harsh\\anaconda3\\envs\\special-llm\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./llama3-medical-wikidoc\\\\tokenizer_config.json',\n",
       " './llama3-medical-wikidoc\\\\special_tokens_map.json',\n",
       " './llama3-medical-wikidoc\\\\tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# Load Model and Tokenizer\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Select Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#  Load Model in **8-bit Quantization** for Memory Efficiency\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32, \n",
    "    load_in_8bit=True,  \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Prepare model for LoRA fine-tuning (Reduce memory usage)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA fine-tuning with **Minimal Trainable Parameters**\n",
    "peft_config = LoraConfig(\n",
    "    r=8,  # Increase LoRA rank for more expressive updates\n",
    "    lora_alpha=16,  # Increase alpha to scale LoRA activations\n",
    "    lora_dropout=0.05,  \n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Move LoRA model to correct device\n",
    "model.to(device)\n",
    "\n",
    "#  Load Medical Instruction Dataset\n",
    "dataset = load_dataset(\"medalpaca/medical_meadow_wikidoc\")\n",
    "\n",
    "#  Print available dataset splits\n",
    "print(\"Available dataset splits:\", dataset.keys())\n",
    "\n",
    "#  Split dataset into **train and validation sets**\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)  # 90% Train, 10% Validation\n",
    "\n",
    "# Print new dataset splits\n",
    "print(f\"Train size: {len(dataset['train'])}, Validation size: {len(dataset['test'])}\")\n",
    "\n",
    "# Tokenization function (Ensure Labels Are Correctly Created)\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Format the input text for instruction-tuned fine-tuning.\"\"\"\n",
    "    prompt = f\"### Instruction:\\n{examples['instruction']}\\n\\n\"\n",
    "    if examples['input']:\n",
    "        prompt += f\"### Context:\\n{examples['input']}\\n\\n\"\n",
    "    prompt += f\"### Response:\\n{examples['output']}\"\n",
    "\n",
    "    tokenized = tokenizer(prompt, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # Ensure labels are correct by shifting input_ids left for causal modeling\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "#  Tokenize dataset (Ensures Loss Can Be Computed)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "#  Assign the correct splits\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "val_dataset = tokenized_datasets[\"test\"]  # Previously used train for validation, now using real val set\n",
    "\n",
    "# Data Collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=False)  \n",
    "\n",
    "#  Training Arguments (Aggressive Memory Optimization)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-medical-wikidoc\",\n",
    "    per_device_train_batch_size=1,  # **Minimal batch size to prevent OOM**\n",
    "    per_device_eval_batch_size=1,  # **Minimal batch size**\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=500,  # **Evaluate every 500 steps**\n",
    "    save_steps=500,\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,  # Slightly higher learning rate for small batch sizes\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "#  Trainer Setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,  #  Correct validation dataset\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "#  Train the model\n",
    "trainer.train()\n",
    "\n",
    "#  Save the fine-tuned model\n",
    "model.save_pretrained(\"./llama3-medical-wikidoc\")\n",
    "tokenizer.save_pretrained(\"./llama3-medical-wikidoc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128257, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model_path = \"./llama3-medical-wikidoc\"  # Path where you saved the trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Ensure tokenizer is consistent with model\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Load model with correct precision\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# **Resize model embeddings to match tokenizer**\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç **Medical Response:**\n",
      "Can you provide an overview of the symptoms of pneumonia? What are the causes of pneumonia?\n",
      "Pneumonia is an infection of the lungs. It is caused by bacteria, viruses, fungi, or parasites. The causes of pneumonia are many and varied. The most common cause of pneumonia is a virus. Other causes of pneumonia include bacteria, fungi, and parasites. The symptoms of pneumonia are many and varied. The most common symptom of pneumonia is a cough. Other symptoms of pneumonia include fever, chills, chest pain, shortness of breath, and fatigue. The causes of pneumonia are many and varied. The most common cause of pneumonia is a virus. Other causes of pneumonia include bacteria, fungi, and parasites. The symptoms of pneumonia are many and varied. The most common symptom of pneumonia is a cough. Other symptoms of pneumonia include fever, chills, chest pain, shortness of breath, and fatigue. The causes of pneumonia are many and varied. The most common cause of pneumonia is a virus. Other causes of pneumonia include bacteria, fungi, and parasites. The symptoms of pneumonia are many and varied. The most common symptom of pneumonia is a cough. Other symptoms of pneumonia include fever, chills, chest pain, shortness of breath, and fatigue. The\n"
     ]
    }
   ],
   "source": [
    "def generate_response(prompt, max_length=256):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,  # Enable randomness for diverse responses\n",
    "            temperature=0.1,  # Lower temp makes it more factual\n",
    "            top_p=0.9,  # Controls diversity\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "query = \"Can you provide an overview of the symptoms of pneumonia?\"\n",
    "response = generate_response(query)\n",
    "\n",
    "print(\"\\nüîç **Medical Response:**\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç **Medical Response:**\n",
      "Can you provide an overview of the lung's squamous cell carcinoma? What are the symptoms of this disease?\n",
      "Answer: Squamous cell carcinoma is a type of lung cancer that is the most common type of lung cancer. It is also the most common type of lung cancer in men. Squamous cell carcinoma is a type of lung cancer that is the most common type of lung cancer. It is also the most common type of lung cancer in men. Squamous cell carcinoma is a type of lung cancer that is the most common type of lung cancer. It is also the most common type of lung cancer in men. Squamous cell carcinoma is a type of lung cancer that is the most common type of lung cancer. It is also the most common type of lung cancer in men. Squamous cell carcinoma is a type of lung cancer that is the most common type of lung cancer. It is also the most common type of lung cancer in men. Squamous cell carcinoma is a type of lung cancer that is the most common type of lung cancer. It is also the most common type of lung cancer in men. Squamous cell carcinoma is a type of lung cancer that is the most common type of lung cancer. It is also the most common type of lung cancer in men.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"Can you provide an overview of the lung's squamous cell carcinoma?\"\n",
    "response = generate_response(query)\n",
    "\n",
    "print(\"\\nüîç **Medical Response:**\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
