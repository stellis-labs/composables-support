{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç **Medical Response (Base Model):**\n",
      "Generate spark code to add 100 of numbers in a list\n",
      "I have a list of numbers and I want to add 100 of them in a list.\n",
      "I have tried to use the following code but it doesn't work:\n",
      "val list = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
      "val list2 = list.take(100)\n",
      "val list3 = list2 ++ list\n",
      "val list4 = list3 ++ list\n",
      "val list5 = list4 ++ list\n",
      "val list6 = list5 ++ list\n",
      "val list7 = list6 ++ list\n",
      "val list8 = list7 ++ list\n",
      "val list9 = list8 ++ list\n",
      "val list10 = list9 ++ list\n",
      "val list11 = list10 ++ list\n",
      "val list12 = list11 ++ list\n",
      "val list13 = list12 ++ list\n",
      "val list14 = list13 ++ list\n",
      "val list15 = list14 ++ list\n",
      "val list16 = list15 ++ list\n",
      "val list17 = list16 ++ list\n",
      "val list18 = list17 ++ list\n",
      "val list19 = list18 ++ list\n",
      "val list20 = list19 ++ list\n",
      "val list21 =\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the **base model** (without fine-tuning)\n",
    "base_model_id = \"meta-llama/Llama-3.2-1B\"  # Base model from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "\n",
    "# Ensure tokenizer is consistent\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Avoids padding issues\n",
    "\n",
    "# Load model with correct precision\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# **Ensure model embeddings match tokenizer**\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def generate_response(prompt, max_length=256):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)  # ‚úÖ Explicitly pass attention mask\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,  # ‚úÖ Fixes warning\n",
    "            max_length=max_length,\n",
    "            do_sample=True,  # Enable randomness for diverse responses\n",
    "            temperature=0.3,  # Higher temp allows more creativity\n",
    "            top_p=0.9,  # Controls diversity\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# **Test with a medical query**\n",
    "query = \"what is x^2+2x+3, find a and b\"\n",
    "response = generate_response(query)\n",
    "\n",
    "print(\"\\nüîç **Medical Response (Base Model):**\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç **Medical Response (Base Model):**\n",
      "what is x^2+2x+3, find a and b?\n",
      "What is the equation of the line that passes through the point (2, 4) and is parallel to the line 3x+2y-5=0?\n",
      "What is the equation of the line that passes through the point (2, 4) and is parallel to the line 3x+2y-5=0?\n",
      "What is the equation of the line that passes through the point (2, 4) and is parallel to the line 3x+2y-5=0?\n",
      "What is the equation of the line that passes through the point (2, 4) and is parallel to the line 3x+2y-5=0?\n",
      "What is the equation of the line that passes through the point (2, 4) and is parallel to the line 3x+2y-5=0?\n",
      "What is the equation of the line that passes through the point (2, 4) and is parallel to the line 3x+2y-5=0?\n",
      "What is the equation of the line that passes through the point (2, 4) and is parallel to the line 3x+2\n"
     ]
    }
   ],
   "source": [
    "# **Test with a medical query**\n",
    "query = \"what is x^2+2x+3, find a and b\"\n",
    "response = generate_response(query)\n",
    "\n",
    "print(\"\\nüîç **Medical Response (Base Model):**\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available dataset splits: dict_keys(['train'])\n",
      "Train size: 9000, Validation size: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\anaconda3\\envs\\special-llm\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_22620\\27437124.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "c:\\Users\\harsh\\anaconda3\\envs\\special-llm\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\harsh\\anaconda3\\envs\\special-llm\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./llama3-medical-wikidoc\\\\tokenizer_config.json',\n",
       " './llama3-medical-wikidoc\\\\special_tokens_map.json',\n",
       " './llama3-medical-wikidoc\\\\tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# Load Model and Tokenizer\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Select Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#  Load Model in **8-bit Quantization** for Memory Efficiency\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32, \n",
    "    load_in_8bit=True,  \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Prepare model for LoRA fine-tuning (Reduce memory usage)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA fine-tuning with **Minimal Trainable Parameters**\n",
    "peft_config = LoraConfig(\n",
    "    r=8,  # Increase LoRA rank for more expressive updates\n",
    "    lora_alpha=16,  # Increase alpha to scale LoRA activations\n",
    "    lora_dropout=0.05,  \n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Move LoRA model to correct device\n",
    "model.to(device)\n",
    "\n",
    "#  Load Medical Instruction Dataset\n",
    "dataset = load_dataset(\"medalpaca/medical_meadow_wikidoc\")\n",
    "\n",
    "#  Print available dataset splits\n",
    "print(\"Available dataset splits:\", dataset.keys())\n",
    "\n",
    "#  Split dataset into **train and validation sets**\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)  # 90% Train, 10% Validation\n",
    "\n",
    "# Print new dataset splits\n",
    "print(f\"Train size: {len(dataset['train'])}, Validation size: {len(dataset['test'])}\")\n",
    "\n",
    "# Tokenization function (Ensure Labels Are Correctly Created)\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Format the input text for instruction-tuned fine-tuning.\"\"\"\n",
    "    prompt = f\"### Instruction:\\n{examples['instruction']}\\n\\n\"\n",
    "    if examples['input']:\n",
    "        prompt += f\"### Context:\\n{examples['input']}\\n\\n\"\n",
    "    prompt += f\"### Response:\\n{examples['output']}\"\n",
    "\n",
    "    tokenized = tokenizer(prompt, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # Ensure labels are correct by shifting input_ids left for causal modeling\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "#  Tokenize dataset (Ensures Loss Can Be Computed)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "#  Assign the correct splits\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "val_dataset = tokenized_datasets[\"test\"]  # Previously used train for validation, now using real val set\n",
    "\n",
    "# Data Collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=False)  \n",
    "\n",
    "#  Training Arguments (Aggressive Memory Optimization)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-medical-wikidoc\",\n",
    "    per_device_train_batch_size=1,  # **Minimal batch size to prevent OOM**\n",
    "    per_device_eval_batch_size=1,  # **Minimal batch size**\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=500,  # **Evaluate every 500 steps**\n",
    "    save_steps=500,\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,  # Slightly higher learning rate for small batch sizes\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "#  Trainer Setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,  #  Correct validation dataset\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "#  Train the model\n",
    "trainer.train()\n",
    "\n",
    "#  Save the fine-tuned model\n",
    "model.save_pretrained(\"./llama3-medical-wikidoc\")\n",
    "tokenizer.save_pretrained(\"./llama3-medical-wikidoc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09743493e8674e6ba22ee79fc52717d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\anaconda3\\envs\\special-llm\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\harsh\\.cache\\huggingface\\hub\\datasets--medalpaca--medical_meadow_medical_flashcards. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6a3b50b7894281bba5eca5cd0bee93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)l_meadow_wikidoc_medical_flashcards.json:   0%|          | 0.00/17.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4282a1b56c364b8f948123c7a90d468d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/33955 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "dataset = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset[\"train\"].to_pandas().head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input          What is the relationship between very low Mg2+...\n",
      "output         Very low Mg2+ levels correspond to low PTH lev...\n",
      "instruction                      Answer this question truthfully\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç **Medical Response (Fine-Tuned Model):**\n",
      "Can you provide an overview of the symptoms of pneumonia? What causes pneumonia?\n",
      "Pneumonia is an infection of the lungs. It is caused by an infection of the respiratory tract, such as a virus or bacteria. The infection causes inflammation of the lungs, which results in the production of mucus and thickened secretions. The mucus and secretions cause the patient to cough and produce sputum. The sputum can be yellow, green, or brown in color. The mucus and secretions can also cause the patient to have difficulty breathing. The patient may also have a fever, chills, and body aches. The patient may also have a cough that produces a thick, white or yellow substance. The cough may also produce blood or pus. The cough may also produce a large amount of sputum. The cough may also produce a large amount of sputum. The cough may also produce a large amount of sputum. The cough may also produce a large amount of sputum. The cough may also produce a large amount of sputum. The cough may also produce a large amount of sputum. The cough may also produce a large amount of sputum. The cough may also produce a large amount of\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# ‚úÖ Load the fine-tuned model path\n",
    "model_path = \"./llama3-medical-wikidoc\"  # Path where you saved the fine-tuned model\n",
    "base_model_id = \"meta-llama/Llama-3.2-1B\"  # The original base model\n",
    "\n",
    "# ‚úÖ Load the tokenizer from the fine-tuned model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# ‚úÖ Ensure the tokenizer has a pad token to avoid warnings\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Avoids padding issues\n",
    "\n",
    "# ‚úÖ Select device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ‚úÖ Load the base model first\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# ‚úÖ Load the fine-tuned LoRA adapter **on top of the base model**\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "\n",
    "# ‚úÖ Ensure model embeddings match tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# ‚úÖ Move model to correct device\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def generate_response(prompt, max_length=256):\n",
    "    \"\"\"Generates a response using the fine-tuned model.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)  # ‚úÖ Explicitly pass attention mask\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,  # ‚úÖ Fixes warning\n",
    "            max_length=max_length,\n",
    "            do_sample=True,  # Enable randomness for diverse responses\n",
    "            temperature=0.4,  # Lower temperature for factual responses\n",
    "            top_p=0.9,  # Controls diversity\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# **Test the fine-tuned model with a medical query**\n",
    "query = \"Can you provide an overview of the symptoms of pneumonia?\"\n",
    "response = generate_response(query)\n",
    "\n",
    "print(\"\\nüîç **Medical Response (Fine-Tuned Model):**\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç **Medical Response:**\n",
      "Can you provide an overview of the symptoms of pneumonia? What are the causes of pneumonia?\n",
      "Pneumonia is an infection of the lungs. It is caused by bacteria, viruses, fungi, or parasites. The causes of pneumonia are many and varied. The most common cause of pneumonia is a virus. Other causes of pneumonia include bacteria, fungi, and parasites. The symptoms of pneumonia are many and varied. The most common symptom of pneumonia is a cough. Other symptoms of pneumonia include fever, chills, chest pain, shortness of breath, and fatigue. The causes of pneumonia are many and varied. The most common cause of pneumonia is a virus. Other causes of pneumonia include bacteria, fungi, and parasites. The symptoms of pneumonia are many and varied. The most common symptom of pneumonia is a cough. Other symptoms of pneumonia include fever, chills, chest pain, shortness of breath, and fatigue. The causes of pneumonia are many and varied. The most common cause of pneumonia is a virus. Other causes of pneumonia include bacteria, fungi, and parasites. The symptoms of pneumonia are many and varied. The most common symptom of pneumonia is a cough. Other symptoms of pneumonia include fever, chills, chest pain, shortness of breath, and fatigue. The\n"
     ]
    }
   ],
   "source": [
    "def generate_response(prompt, max_length=256):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,  # Enable randomness for diverse responses\n",
    "            temperature=0.1,  # Lower temp makes it more factual\n",
    "            top_p=0.9,  # Controls diversity\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "query = \"Can you provide an overview of the symptoms of pneumonia?\"\n",
    "response = generate_response(query)\n",
    "\n",
    "print(\"\\nüîç **Medical Response:**\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç **Medical Response:**\n",
      "Can you provide an overview of the lung's squamous cell carcinoma? What are the symptoms of this disease?\n",
      "Answer: Squamous cell carcinoma is a type of lung cancer that is the most common type of lung cancer. It is also the most common type of lung cancer in men. Squamous cell carcinoma is a type of lung cancer that is the most common type of lung cancer. It is also the most common type of lung cancer in men. Squamous cell carcinoma is a type of lung cancer that is the most common type of lung cancer. It is also the most common type of lung cancer in men. Squamous cell carcinoma is a type of lung cancer that is the most common type of lung cancer. It is also the most common type of lung cancer in men. Squamous cell carcinoma is a type of lung cancer that is the most common type of lung cancer. It is also the most common type of lung cancer in men. Squamous cell carcinoma is a type of lung cancer that is the most common type of lung cancer. It is also the most common type of lung cancer in men. Squamous cell carcinoma is a type of lung cancer that is the most common type of lung cancer. It is also the most common type of lung cancer in men.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"Can you provide an overview of the lung's squamous cell carcinoma?\"\n",
    "response = generate_response(query)\n",
    "\n",
    "print(\"\\nüîç **Medical Response:**\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
